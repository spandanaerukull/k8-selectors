Kubernetes Scheduling & Placement Strategies
This repo contains concise, example-driven YAML files that simplify key Kubernetes concepts: node and pod placement using selectors, affinity/anti-affinity rules, and real-world scheduling logic. Ideal for interview prep or visual explanations in content creation.

ğŸ“ File Breakdown
ğŸ“„ Filename	ğŸ” Purpose
01-node-selector.yaml	ğŸ¯ Assign pods to nodes with explicit labels using nodeSelector
02-node-selector.yaml	ğŸ§ª Variant example to reinforce nodeSelector mechanics
03-node-affinity.yaml	â¤ï¸ Use requiredDuringSchedulingIgnoredDuringExecution for strict node affinity
04-node-anti-affinity.yaml	ğŸš« Avoid scheduling pods on certain nodes using node anti-affinity
05-pod-affinity.yaml	ğŸ¤ Co-locate pods for communication or proximity (e.g., cache + app)
06-pod-anti-affinity.yaml	âš–ï¸ Distribute pods to avoid single-point failure (e.g., replicas across zones)
07-use-case.yaml	ğŸ§© Combine multiple placement rules to reflect production-grade deployment
ğŸš€ Quick Mnemonic
Here's a mnemonic to simplify these concepts: ğŸ§  N.A.P.S.U = NodeSelector, Affinity, PodSelector, Spread, Use-case

N = ğŸ¯ nodeSelector â€” direct label match
A = â¤ï¸ Affinity â€” prefer or require certain nodes
P = ğŸ¤ PodAffinity â€” place pods together
S = âš–ï¸ PodAntiAffinity â€” separate replicas across nodes
U = ğŸ§© Use-case â€” real-world combination of strategies
ğŸ”— When to Use What?
Strategy	When to Use
nodeSelector	When placement is simple and label-driven
nodeAffinity	For complex node conditions (preferred vs required)
podAffinity	Co-locating collaborating services (e.g., app â†” cache)
podAntiAffinity	High availability with distributed replicas
